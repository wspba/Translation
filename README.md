ResNet-20
模型结构
ResNet中的Residual block采用了shortcut connection的结构：
 
图3-1 Residual block
在我们的实验中所使用的ResNet包含三种类型的的Residual block，对应的特征图分别为32、16和8，输出通道分别为16、32和64。在ResNet-20中，每种类型的Residual block的个数为3个，在输出通道数量发生变化的Residual block中，在shortcut connection上使用映射来保证element-wise相加的两个feature map维度匹配。具体的结构如下表：
表3-1 ResNet-20结构
Layer name	Output size	Residual block
Conv_1	32×32	[3×3  16]
Conv_2	32×32	[(3×3  16)¦(3×3  16)]×3
Conv_3	16×16	[(3×3  32)¦(3×3  32)]×3
Conv_4	8×8	[(3×3  64)¦(3×3  64)]×3
Avg-pool	1×1	[8×8]
模型训练
在ResNet原始论文中，作者在训练图像的4条边上均扩展了4个像素，然后再crop成32的大小进行训练，因此我们也采用这一方式， 除此之外不做任何数据预处理和数据增强，只在训练时使用mirror和shuffle，batch-size设为128。初始学习率设为0.1，使用“multistep”的学习率策略，总共训练64000个迭代，在32000次和48000次时将学习率除以10。优化器类型为SGD，momentum为0.9，weight decay为0.0001。
模型效果
ResNet-20在cifar-10数据集上的运算量为0.082Gflops，参数数量为0.272	M，测试准确率为91.35%，测试准确率曲线如下图：
 
图3-2 ResNet-20在cifar-10测试集上的准确率曲线
小结
ResNet-20在cifar-10数据集上的运算量和参数数量都很低，但是准确率离要求的94%还有一定的差距，但是考虑到该实验中没有使用任何数据增强，因此下一步考虑数据增强的手段来提高模型的效果。




 数据增强
数据增强的方式
在深度学习中，为了丰富图像训练集，增加样本的多样性，更好的提取图像特征，提高模型的性能并减少模型的过拟合，往往都会采取数据增强的方式。
AlexNet最早提出了一种PCA的数据增强方式，通过计算训练图像三个通道上像素值的主成分，然后将得到的特征值乘以一个服从高斯分布的随机值，再加到训练图像三个通道上的像素值上，作者提到使用这种方法可以将训练错误率降低1%以上。海康研究院在2016年ImageNet上也使用了这种pca抖动的方法。
除此之外，常用的数据增强方式还有随机旋转变换、随机翻转、缩放变换、平移变换、颜色变换、对比度变换、噪声扰动、平滑模糊等。考虑到cifar-10数据集图像尺寸小、种类简单，因此使用太过复杂的数据增强，不一定会给模型在该数据集上的效果带来提升，因此，需要筛选在模型训练时使用怎样的数据增强技术。
数据增强方式的选择
通过观察数据集图像发现，所有训练集和测试集图像的朝向都是同一个方向，因此不使用随机翻转的数据增强方式，使用随机旋转时，将旋转角度控制在-10到10度之间。原始图像尺寸为32*32，通过填充后，变成40*40，在随机缩放时，将缩放的尺寸设为36到44之间。同时考虑到图像尺寸较小，像素值的信息较少，因此不使用三个通道上颜色的变换，使用对比度变换、噪声扰动以及平滑模糊时，只使用比较小的扰动。因此该实验中选择的数据增强方式有：随机pca抖动、随机仿射变换、随机对比度变换、随机尺度缩放、随机高斯噪声、随机模糊平滑。为了快速验证数据增强的效果，每次增加一种数据增强方式，来观测该种数据增强方式给模型训练带来的影响。
数据增强的效果
依然使用ResNet-20的模型和相同的实验参数设置，观测在cifar-10数据集上的测试准确率曲线如下图所示：
 
图3-3 使用数据增强时ResNet-20在cifar-10测试集上的准确率曲线
图中base line代表不使用任何数据增强时的效果，p代表随机pca抖动，a代表随机仿射变换，c代表随机对比度变换，s代表随机尺度缩放，g代表随机高斯噪声，b代表随机模糊平滑。实验发现前四种数据增强方式都能给模型的效果带来一定的提升，而随机高斯噪声和随机模糊平滑则使模型的效果降低，因此我们选择使用pca抖动、仿射变换、对比度变换、随机缩放这四种数据增强方式，在使用这四种数据增强的情况下，ResNet-20在cifar-10数据集上的测试准确率达到了93.11%。
小结
通过数据增强的方式，在不改变模型运算量和参数数量的情况下，将测试准确率从91.35%提升到93.11%，效果很明显，但是依然没有达到94%的要求。下一步考虑从模型的结构入手，通过改变模型的结构来提升模型的性能。




 Wide ResNet
模型结构
在《Wide Residual Networks》一问中，作者通过将ResNet卷积层的宽度增加，达到了很好的性能效果，因此我们参考Wide ResNet的方法，在ResNet-20的基础上，通过增加网络卷积层输出通道的数量来修改模型。在这里，我们考虑使用2、4、8倍宽度的WRNS-20，为了方便，分别记为WRNS(20,2)、WRNS(20,4)和WRNS(20,8)模型的具体结构如下表：
表3-2 Wide ResNet-20结构
Layer name	Output size	Residual block (N=2,4,8)
Conv_1	32×32	[3×3  16]
Conv_2	32×32	[(3×3  16×N)¦(3×3  16×N)]×3
Conv_3	16×16	[(3×3  32×N)¦(3×3  32×N)]×3
Conv_4	8×8	[(3×3  64×N)¦(3×3  64×N)]×3
Avg-pool	1×1	[8×8]
模型训练
参考Wide ResNet论文，在本部分实验中，将初始学习率设为0.1，使用“multistep”的学习率策略，总共训练144000个迭代，在48000次、84000次和102000次时将学习率除以5。优化器类型为Nesterov，momentum为0.9，weight decay为0.0005。使用与上节内容中相同的数据增强方式，batch-size为128。
模型效果
WRNS(20,2)在cifar-10上的运算量为0.315Gflops，参数数量为1.079M，测试准确率为95.77%；WRNS(20,4)在cifar-10上的运算量为1.238Gflops，参数数量为4.297M，测试准确率为96.41%；WRNS(20,8)在cifar-10上的运算量为4.908Gflops，参数数量为17.147M，测试准确率为96.73%。测试准确率曲线如下图：
 
图3-4 不同宽度的Wide ResNet-20在cifar-10测试集上的准确率曲线
小结
通过以上实验发现，当模型的宽度增加时，模型的性能确实能够得到有效的提升，但是随着宽度的进一步加大时（比如，由4倍到8倍时），性能的提升有限，但是在运算量和参数数量上都增加了很多，而且模型在训练时消耗的时间和内存都要大很多，因此针对不同的训练集以及不同的需求，选择合理的模型宽度上非常重要的。
Bottleneck
Bottleneck的结构
在ResNet原文中，作者提出了一种bottleneck的结构，将每一个Residual block中的两个3*3的卷积层换成1*1、3*3、1*1这样三个卷积层，第一个1*1的卷积层用来实现降维的功能，最后一个1*1的卷积层用来恢复维度，这样做降低了3*3的卷积层的输入维度，大大降低了计算量和参数量，同时参考NIN的结构，1*1的卷积融合了多个channel的特征信息，能够提高模型的表达能力。因此本部分，我们尝试在WRNS(20,4)和WRNS(20,8)上使用bottleneck的结构来降低模型的运算量和参数量。
考虑到如果维度降低太多，可能对模型性能的影响比较严重，因此分别使用以下两种bottleneck的结构，两种结构从不同程度上对卷积层的输入进行降维来降低模型的运算量和参数量：
 
图3-5 bottleneck的Residual block
	模型的具体结构如下表：
表3-3 Wide ResNet-20结构
Layer name	Output size	Residual block-A (N=4,8)	Residual block-B (N=4,8)
Conv_1	32×32	[3×3  16]	[3×3  16]
Conv_2	32×32	[█(1×1  4×N@3×3  4×N)¦(1×1  16×N)]×3	[█(1×1  8×N@3×3  8×N)¦(1×1  16×N)]×
Conv_3	16×16	[█(1×1  8×N@3×3  8×N)¦(1×1  32×N)]×3	[█(1×1  16×N@3×3  16×N)¦(1×1  32×N)]×3
Conv_4	8×8	[█(1×1  16×N@3×3  16×N)¦(1×1  64×N)]×3	[█(1×1  32×N@3×3  32×N)¦(1×1  64×N)]×3
Avg-pool	1×1	[8×8]	[8×8]

模型训练
模型的训练方式与3.3节中的训练方式一样。
模型效果
WRNS(20,4)-A在cifar-10上的运算量为0.088Gflops，参数数量为0.308M，测试准确率为94.11%；WRNS(20,4)-B在cifar-10上的运算量为0.249Gflops，参数数量为0.861M，测试准确率为95%；WRNS(20,8)-A在cifar-10上的运算量为0.344Gflops，参数数量为1.223M，测试准确率为95.29%；WRNS(20,8)-B在cifar-10上的运算量为0.989Gflops，参数数量为3.437M，测试准确率为96.15%。测试准确率曲线如下图：
 
图3-4 使用bottleneck结构的WRNS在cifar-10测试集上的准确率曲线
小结
我们发现使用bottleneck结构的WRNS(20,4)-A，在运算量和参数量都仅和原始的ResNet-20基本一样的情况下（运算量0.088vs0.082，参数量0.308vs0.272），在cifar-10数据集上的分类准确率达到了94.11%（原始ResNet-20为93.11%），这也是目前我所有实验中，在保证分类准确率在94%以上，运算量和参数量都最低的模型。而WRNS(20,8)-B在运算量和参数量都只有原始WRNS(20,8)的5分之1时，分类准确率依旧达到了96.15%。


 模型压缩
由于在cifar-10数据集上，WRNS(20,8)和WRNS(20,4)相比，分类准确率提升有限（0.3%），而运算量和参数量都增大4倍左右，并且训练的时间也长很多。考虑到时间的限制，我们就在WRNS(20,4)上尝试进行模型压缩的实验，我们希望在保证模型性能的基础上，进一步压缩它的运算量和参数量。
模型压缩方式
目前常用的模型压缩方式有：权值量化、权值裁剪、稀疏化等方式。在这里，主要参考2017年ICLR的一篇文章《Pruning Filters for Efficient Convnets》，主要方法是统计卷积层中每个filter的weight的大小在该层所有filter中所占的比例，认为比例较小的filter在该层中所起到的作用也是较小的，因此可以剔除这些filter来压缩模型。
模型压缩实验
上面提到的基于filter的裁剪方法有两种实验策略，I：将模型所有层的filter都裁剪完，再重新进行训练；II：从上往下，裁剪完一层后，重新训练，训练完后再继续裁剪后面的层。接下来我们对这两种策略进行实验，在训练好的WRNS(20,4)上进行裁剪，所有实验中，filter裁剪的阈值相同。
当对所有层采用方案I时，模型的运算量降为0.387Gflops，参数量降为1.51M，重新训练后在cifar-10数据集上的测试准确率为95.34%； 当采用方案II时，模型的运算量降为0.397Gflops，参数量降为1.57M，重新训练后在cifar-10数据集上的测试准确率为95.47%。我们可以看出，使用逐层裁剪训练的方法，比一次性全部裁剪再训练的方法，效果要稍微好一点。但是总的来看，虽然通过裁剪，模型的运算量和参数量都降低了很多，但是测试的准确率也降低了很多。
考虑到ResNet结构的特殊性：它是由三类输出通道数量不同的Residual block组成（从上往下分别记为A、B、C三类）， Residual block具有shortcut connection的结构，因此在每一类输出通道数量的Residual block中，前面层filter的裁剪将会影响到该类后面所有层以及下一类前两层的filter，因此我们考虑只裁剪部分层的filter来进行实验。
当只裁剪A类和B类Residual block的filter时，使用方案I时，模型的运算量降为0.630Gflops，参数量降为3.413M，重新训练后在cifar-10数据集上的测试准确率为95.76%；使用方案II时，模型的运算量降为0.633Gflops，参数量降为3.418M，重新训练后在cifar-10数据集上的测试准确率为95.92%。
当只裁剪A类和C类Residual block的filter时，使用方案II时，模型的运算量降为0.70Gflops，参数量降为2.46M，重新训练后在cifar-10数据集上的测试准确率为95.89%；使用方案II时，模型的运算量降为0.70Gflops，参数量降为2.46M，重新训练后在cifar-10数据集上的测试准确率为96.11%，基本上达到了模型压缩的目的。
小结
通过逐层裁剪filter再训练的方式，对训练好的WRNS(20,4)进行裁剪，得到的模型的测试准确率为96.11%，而运算量也由之前的1.238Gflops降为0.70Gflops，参数量由之前的4.297M降为2.46M。模型压缩
 方法对比
我们对上述各项实验中，达到准确率要求的所有模型方法，在运算量、参数量、前向时间以及分类准确率上进行一个比较，如下表：
Method	GFlops	Params	Forward Pass	Accuracy
WRNS(20,2)	0.315	1.079M	19.87ms	95.77%
WRNS(20,4)	1.238	4.297M	42.64ms	96.41%
WRNS(20,8)	4.908	17.147M	100.43ms	96.73%
WRNS(20,4)-Bottleneck-A	0.088	0.308M	25.86ms	94.11%
WRNS(20,4)-Bottleneck-B	0.249	0.861M	32.69ms	95%
WRNS(20,8)-Bottleneck-A	0.344	1.233M	46.30ms	95.29%
WRNS(20,8)-Bottleneck-B	0.989	3.437M	60.51ms	96.15%
WRNS(20,4)-Prune-ABC-II
0.387	1.51M	23.63ms	95.34%
WRNS(20,4)-Prune-ABC-II
0.397	1.567M	23.96ms	95.47%
WRNS(20,4)-Prune-AB-II
0.63	3.413M	28.06ms	95.76%
WRNS(20,4)-Prune-AB-II
0.633	3.418M	28.21ms	95.92%
WRNS(20,4)-Prune-AC-II
0.70	2.46M	29.14ms	95.89%
WRNS(20,4)-Prune-AC-II
0.70	2.46M	29.10ms	96.11%
	经过综合比较，我们最终选择使用WRNS(20,4)-Prune-AC-II的模型结构为最优模型。
