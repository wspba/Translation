目前在深度学习领域分类两个派别，一派为学院派，研究强大、复杂的模型网络和实验方法，为了追求更高的性能；另一派为工程派，旨在将算法更稳定、高效的落地在硬件平台上，效率是其追求的目标。复杂的模型固然具有更好的性能，但是高额的存储空间、计算资源消耗是使其难以有效的应用在各硬件平台上的重要原因。目前已有越来越多关于模型压缩方法的研究，从理论研究到平台实现，取得了非常大的进展。

2015年，Han发表的Deep Compression是一篇对于模型压缩方法的综述型文章，将裁剪、权值共享和量化、编码等方式运用在模型压缩上，取得了非常好的效果。目前模型压缩方法的研究主要可以分为以下几个方向。更精细模型的设计，目前的很多网络都具有模块化的设计，在深度和宽度上都很大，这也造成了参数的冗余很多，因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。模型裁剪，结构复杂的网络具有非常好的性能，其参数也存在冗余，因此对于已训练好的模型网络，可以寻找一种有效的评判手段，将不重要的connection或者filter进行裁剪来减少模型的冗余。核的稀疏化，在训练过程中，对权重的更新进行诱导，使其更加稀疏，对于稀疏矩阵，可以使用更加紧致的存储方式，如CSC，但是使用稀疏矩阵操作在硬件平台上运算效率不高，容易受到带宽的影响，因此加速并不明显。除此之外，还有量化、分解、迁移学习等方法，都在模型压缩中起到了非常好的效果。

调研的内容主要分为以下几个方向，核的稀疏化，模型裁剪，精细的网络设计以及迁移学习。各种方法在实际运用中均取得了很好的效果，下面就这些方法分别选取典型算法案进行介绍。

基于核稀疏化的方法
核的稀疏化，是在训练过程中，对权重的更新加以正则项进行诱导，使其更加稀疏，使大部分的权值都为0。核的稀疏化方法分为regular和irregular，regular的稀疏化后，裁剪起来更加容易，尤其是对im2col的矩阵操作，效率更高；而irregular的稀疏化后，参数需要特定的存储方式，或者需要平台上稀疏矩阵操作库的支持，典型的方案如下：

Learning Structured Sparsity in Deep Neural Networks 
提出了一种Structured Sparsity Learning的学习方式，能够学习一个稀疏的结构来降低计算消耗，所学到的结构性稀疏化能够有效的在硬件上进行加速。
传统非结构化的随机稀疏化会带来不规则的内存访问，因此在GPU等硬件平台上无法有效的进行加速。
作者在网络的目标函数上增加了group lasso的限制项，可是实现filter级与channel级以及shape级稀疏化。由于在GEMM中将weight tensor拉成matrix的结构，因此可以通过将filter级与shape级的稀疏化进行结合来将2D矩阵的行和列稀疏化，再分别在矩阵的行和列上裁剪掉剔除全为0的值可以来降低矩阵的维度从而提升模型的运算效率。该方法是regular的方法，压缩粒度较粗，可以适用于各种现成的算法库，但是训练的收敛性和优化难度不确定。

Dynamic Network Surgery for Efficient DNNs 
作者提出了模型裁剪的两个过程：pruning和splicing，其中pruning就是将认为不中要的weight裁掉，但是往往无法直观的判断哪些weight是否重要，因此在这里增加了一个splicing的过程，将哪些重要的被裁掉的weight再恢复回来。
作者通过在W上增加一个T来实现，T为一个2值矩阵，起到的相当于一个mask的功能，当某个位置为1时，将该位置的weight保留，为0时，裁剪。在训练过程中通过一个可学习mask将weight中真正不重要的值剔除，从而使得weight变稀疏。属于irregular的方式，裁剪粒度细，性能较好，但是ak和bk的值在不同的模型以及不同的层中无法确定，并且容易受到稀疏矩阵算法库以及带宽的限制。

Training Skinny Deep Neural Networks with Iterative Hard Thresholding Methods
作者想通过训练一个稀疏度高的网络来降低模型的运算量，通过在网络的损失函数中增加一个关于W的L0范式可以降低W的稀疏度，但是L0范式就导致这是一个N-P难题，是一个难优化求解问题，因此作者从另一个思路来训练这个稀疏化的网络。
先正常训练网络s1轮，然后Ok(W)表示选出W中数值最大的k个数，而将剩下的值置为0，supp(W,k)表示W中最大的k个值的序号，继续训练s2轮，仅更新非0的W，然后再将之前置为0的W放开进行更新，继续训练s1轮，这样反复直至训练完毕。
同样也是对参数进行诱导的方式，边训练边裁剪，先将认为不重要的值裁掉，再通过一个restore的过程将重要却被误裁的参数恢复回来。也是属于irregular的方式，边训边裁，性能不错，压缩的力度难以保证。

方法小结
本章节的三篇文章都是基于核稀疏化的方法，都是在训练过程中，对参数的更新进行限制，使其趋向于稀疏，或者在训练的过程中将不重要的连接截断掉，其中第一篇文章提供了结构化的稀疏化，可以利用GEMM的矩阵操作来实现加速，该文章有开源代码。第二篇文章同样是在权重更新的时候增加限制，和第一篇文章一样，虽然通过对权重的更新进行限制可以很好的达到稀疏化的目的，但是给训练的优化增加了难度，降低了模型的收敛性。此外第二篇和第三篇文章都是非结构化的稀疏化，容易受到稀疏矩阵算法库以及带宽的限制，这两篇文章在截断连接后还使用了一个surgery的过程，能够降低重要参数被裁剪的风险。

基于模型裁剪的方法
对以训练好的模型进行裁剪的方法，是目前模型压缩中使用最多的方法，通常是寻找一种有效的评判手段，来判断参数的重要性，将不重要的connection或者filter进行裁剪来减少模型的冗余。同样也分为regular和irregular的方式。
这类方法最多，下面列举几篇典型的方案。

Pruning Filters for Efficient Convnets
作者提出了基于量级的裁剪方式，用weight值的大小来评判其重要性，对于一个filter，其中所有weight的绝对值求和，来作为该filter的评价指标，将一层中值低的filter裁掉，可以有效的降低模型的复杂度并且不会给模型的性能带来很大的损失。
为了判断每一层对于裁剪的敏感性，作者将每一层的filter的值按大小排序，得到下图左，然后单独裁剪每一层来看裁剪后的准确率。对于裁剪较敏感的层，作者使用更小的裁剪力度，或者跳过这些层不进行裁剪。

Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures
作者认为，在大型的深度学习网络中，大部分的神经元的激活都是趋向于零的，而这些激活为0的神经元是冗余的，将它们剔除可以大大降低模型的大小和运算量，而不会对模型的性能造成影响，于是作者定义了一个量APoZ（Average Percentage of Zeros）来衡量每一个filter中激活为0的值的数量，来作为评价一个filter是否重要的标准。
作者发现在VGG-16中，有631个filter的APoZ超过了90%，也就说明了网络中存在大量的冗余。该方法对于后面的卷积层及全连接层的压缩比较有效，总的压缩力度不够，作者只在最后一个卷积层和全连接层上进行了实验。

An Entropy-based Pruning Method for CNN Compression
作者认为通过weight值的大小很难判定filter的重要性，通过这个来裁剪的话有可能裁掉一些有用的filter。因此作者提出了一种基于熵值的裁剪方式，利用熵值来判定filter的重要性。
作者将每一层的输出通过一个Global average Pooling将feature map转换为一个长度为c（filter数量）的向量，对于n张图像可以得到一个n*c的矩阵，对于每一个filter，将它分为m个bin，统计每个bin的概率，然后计算它的熵值
利用熵值来判定filter的重要性，再对不重要的filter进行裁剪。在retrain中，作者使用了这样的策略，即每裁剪完一层，通过少数几个迭代来恢复部分的性能，当所有层都裁剪完之后，再通过较多的迭代来恢复整体的性能。
由于作者考虑VGG-16全连接层所占的参数量太大，因此使用GAP的方式来降低计算量。该裁剪方法对于最后几个卷积层的裁剪力度较小，可能与后面卷积层的熵值分布有关，也可能与对该方法的敏感性有关。
作者将自己的retrain方式与传统的retrain方式进行比较，发现作者的方法能够有效的减少retrain的步骤，并也能达到不错的效果。

Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning
作者认为以往的裁剪方法，都没有考虑到模型的带宽以及能量的消耗，因此无法从能量利用率上最大限度的裁剪模型，因此提出了一种基于能量效率的裁剪方式。
作者指出一个模型中的能量消耗包含两个部分，一部分是计算的能耗，一部分是数据转移的能耗，在作者之前的一片论文中（与NVIDIA合作，Eyeriss），提出了一种估计硬件能耗的工具，能够对模型的每一层计算它们的能量消耗。然后将每一层的能量消耗从大到小排序，对能耗大的层优先进行裁剪，这样能够最大限度的降低模型的能耗，对于需要裁剪的层，根据weight的大小来选择不重要的进行裁剪，同样的作者也考虑到不正确的裁剪，因此将裁剪后模型损失最大的weight保留下来。每裁剪完一层后，对于该层进行locally的fine-tune，locally的fine-tune，是在每一层的filter上，使用最小二乘优化的方法来使裁剪后的filter调整到使得输出与原始输出尽可能的接近。作者从能耗的角度对层的裁剪程度进行分析，经过裁剪后，模型的能耗压缩较大，但是性能降低较多。可以参考文章的能耗评定工具。

Coarse Pruning of Convolutional Neural Networks with Random Masks
作者认为，既然我无法直观上的判定filter的重要性，那么就采取一种随机裁剪的方式，然后对于每一种随机方式统计模型的性能，来确定局部最优的裁剪方式。
这种随机裁剪方式类似于一个随机mask，假设有M个潜在的可裁剪weight，那么一共就有2^M个随机mask。假设裁剪比例为a，那么每层就会随机选取ML*a个filter，一共随机选取N组组合，然后对于这N组组合，统计裁剪掉它们之后模型的性能，然后选取性能最高的那组作为局部最优的裁剪方式。
作者同样将这种裁剪方式运用在kernel上，对kernel进行裁剪，并将filter级和kernel级的裁剪进行组合，得到了较好的结果。
方法简单粗暴，看起来也比较有效，没有考虑每一层对于裁剪的敏感性，也没有评价参数的重要性，可能需要尝试多次才能得到较好的结果。

Efficient Gender Classification Using a Deep LDA-Pruned Net
作者发现，在最后一个卷积层中，经过LDA分析发现对于每一个类别，有很多filter之间的激活是高度不相关的，因此可以利用这点来剔除大量的只具有少量信息的filter而不影响模型的性能。
作者在VGG-16上进行实验，VGG-16的conv5_3具有512个filter，将每一个filter的输出值中的最大值定义为该filter的fire score，因此对应于每一张图片就具有一个512维的fire向量，当输入一堆图片时，就可以得到一个N*512的fire矩阵，作者用intra-class correlation来衡量filter的重要性。
作者这样做的目的是通过只保留对分类任务提取特征判别性最强的filter，来降低模型的冗余。
在前面几个卷积层，由于提取的特征都是很简单的特征，如边缘、颜色，直接对它们求ICC可能难以达到好的效果，因此作者在这些层之后接了deconv层，将特征映射到pixel级别，再来计算。
对于VGG最后用来分类的全连接层，作者认为参数量太大，故用Bayesian或者SVM来替代，在目前的ResNet或者Inception中不存在这样的问题，可以不用考虑。

Sparsifying Neural Network Connections for Face Recognition
作者认为，如果一层中的某个神经元的激活与上一层的某个神经元的激活有很强的相关性，那么这个神经元对于后面层的激活具有很强的判别性。也就是说，如果前后两层中的某对神经元的激活具有较高的相关性，那么它们之间的连接weight就是非常重要的，而弱的相关性则代表低的重要性。如果某个神经元可以视为某个特定视觉模式的探测器，那么与它正相关的神经元也提供了这个视觉模式的信息，而与它负相关的神经元则帮助减少误报。作者还认为，那些相关性很低的神经元对，它们之间的连接不一定是一点用也没有，它们可能是对于高相关性神经元对的补充。
作者提供了一个基于神经元激活相关性的重要性判别方法，属于irregular的方法，在计算公式上增加一个求和项可以将这种方法拓展到filter级上。但是作者在实验中并没有对所有的层进行裁剪来比较效果。

Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning Inference
作者将裁剪问题当做一个组合优化问题：从众多的权重参数中选择一个最优的组合B，使得被裁剪的模型的代价函数的损失最小。

Faster CNNs with Direct Sparse Convolutions and Guided Pruning
作者指出，由于传统的稀疏矩阵的卷积运算在不同平台上的计算效率是不一样的，容易受到内存带宽的限制，因此速度的提升并不明显，因此已有的裁剪方法为了适应硬件，往往采取的是group级的裁剪方式，而这种粗糙的裁剪方式往往会带来较大的性能退化，因此很难在性能和裁剪力度上进行权衡。
根据传统稀疏矩阵操作中存在的不足，作者提出了一种新的稀疏矩阵操作方式，能够高性能的对稀疏矩阵进行运算，并能够达到精细的元素级裁剪，稀疏矩阵的卷积操作可以转换成如下图的方式：
将带宽考虑到计算当中，作者发现只有在稀疏度达到某个程度时，在硬件上才能够有效的提速，即当weight中非0值的比例小于一定值（如0.2）时，才能获得明显的加速效果。这样就可以来指导我们进行裁剪，因为在某些层，裁剪的稀疏度达不到一定程度时，起不到加速效果，因此可以跳过这些层，而将裁剪力度运用在其他层上，而达到一个更好的效果。

总的来看，#1实现较简单；#2和#3文章所使用的方法具有比较好的实验结果，它们的评价标准和裁剪策略都值得借鉴，尤其是#3的retrain方式相比较其他方法节省了训练时间；#4文章所使用的硬件能耗评估工具可以借鉴使用；#5文章方法随机性较强，对于层数更多参数量更大的网络，需要尝试更多的mask；#6文章方法实现较复杂，尤其是对于最后一个卷积层之前的层的具体实现方式不清楚，并且对于对标签或者其他任务可用性不大；#7和#8都为参数重要性的评价标准提供了新的思路，但是实现起来复杂度都较高。

基于迁移学习（教师—学生网络）的方法
迁移学习也就是将一个模型的性能迁移到另一个模型上，在这里也叫做教师-学生模型，教师网络往往是一个更加复杂的网络，具有非常好的性能和泛化能力，可以用这个网络来作为一个soft target来指导另外一个更加简单的学生网络来学习，使得简单的模型也能够具有和教师网络相近的性能，也算是一种模型压缩的方式。

Distilling the Knowledge in a Neural Network
较大、较复杂的网络虽然通常具有很好的性能，但是也存在很多的冗余信息，因此运算量以及资源的消耗都非常多。而所谓的Distilling就是将复杂网络中的有用信息提取出来迁移到一个更小的网络上，这样学习来的小网络可以具备和大的复杂网络想接近的性能效果，并且也大大的节省了计算资源。这个复杂的网络可以看成一个教师，而小的网络则可以看成是一个学生。
这个复杂的网络是提前训练好具有很好性能的网络，学生网络的训练含有两个目标：一个是hard target，即原始的目标函数，为小模型的类别概率输出与label真值的交叉熵；另一个为soft target，为小模型的类别概率输出与大模型的类别概率输出的交叉熵，在soft target中，概率输出的公式调整如下，这样当T值很大时，可以产生一个类别概率分布较缓和的输出：
作者认为，由于soft target具有更高的熵，它能比hard target提供更加多的信息，因此可以使用较少的数据以及较大的学习率。将hard和soft的target通过加权平均来作为学生网络的目标函数，soft target所占的权重更大一些。
作者同时还指出，T值取一个中间值时，效果更好，而soft target所分配的权重应该为T^2，hard target的权重为1。
这样训练得到的小模型也就具有与复杂模型近似的性能效果，但是复杂度和计算量却要小很多。

Face Model Compression by Distilling Knowledge from Neurons
作者指出，虽然Hinton提出的Distilling方法能够有效的将一个复杂模型的性能迁移到小的模型上，使小的模型在只具有很低运算量的前提下依然具有和复杂模型近似的性能，但是这种方法在人脸识别中却很难应用，因为人脸识别往往涉及大量的身份（上万）识别任务，对应了上万个label，如果要计算soft target的话会很困难，让收敛变得很慢。因此作者提出了一种新的方法，使用最后一个隐层的输出来监督小模型的训练，作者认为最后一个隐层的输出比soft target具有更多的信息，而且维度上也小很多，并且作者还提出了一种神经元的选择方式，挑选更具有判别力的输出来监督小模型，这样维度进一步降低。
而神经元的选择方式如下：
通过这个惩罚函数，作者希望挑选具有较强判别力且与其他神经元输出相关性很低的神经元。
在人脸识别数据集中，除了身份标注，每张图上还有属性标注，这些属性标注中有与身份相关的属性（IA），也有与身份标注无关的属性（NA），作者用最后一个隐层的每一个神经元来对每一个属性进行分类，对IA属性分类准确率较高的神经元则被认为是具有较强判别力的神经元，因此就挑选这部分来进行学习。
通过这种方法，作者很好的解决了传统Distilling在人脸识别上存在的问题，有效的将复杂模型的训练迁移到小的网络上。本文中，神经元选择的方式值得借鉴。

Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer
作者借鉴Distilling的思想，使用复杂网络中能够提供视觉相关位置信息的Attention map来监督小网络的学习，并且结合了低、中、高三个层次的特征，示意图如下：
教师网络从三个层次的Attention Transfer对学生网络进行监督。其中三个层次对应了ResNet中三组Residual Block的输出。在其他网络中可以借鉴。
这三个层次的Attention Transfer基于Activation，Activation Attention为feature map在各个通道上的值求和，基于Activation的Attention Transfer的损失函数如下：
作者提出在这里在这里对Q进行标准化对于学生网络的训练非常重要。
除了基于Activation的Attention Transfer，作者还提出了一种Gradient Attention，它的损失函数如下：
但是就需要两次反向传播的过程，实现起来较困难并且效果提升不明显。
基于Activation的Attention Transfer效果较好，而且可以和Hinton的Distilling结合。目前已有基于Activation的实现源码。

方法小结
教师学生网络的方法，利用一个性能较好的教师网络，在神经元的级别上，来监督学生网络的训练，相当于提高了模型参数的利用率。在训练过程中需要添加神经元级别的损失函数。本节的第二篇文章提供了一种重要神经元的选择方式，降低了迁移学习的维度。

基于精细模型设计的方法
上述几种方法都是在已有的性能较好模型的基础上，在保证模型性能的前提下尽可能的降低模型的复杂度以及运算量。除此之外，还有很多工作将注意力放在更小、更高效、更精细的网络模块设计上，如SqueezeNet的fire module，ResNet的Residual module，GoogLenet的Inception Module，它们基本都是由很小的卷积（1*1和3*3）组成，不仅参数运算量小，同时还具备了很好的性能效果。下面列举这两年较新的方案。

MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
MobileNet的思路源自于Google之前的一项工作Xception，它在feature map的每一个通道上单独进行3*3的卷积来提取空间信息，称为Depthwise Separable Convolutions，然后再使用1*1的卷积将多个通道的信息线性组合起来，称为Pointwise Convolutions，可以看成是Inception的一个极端情况，可以很大程度的降低计算量，并且也具有不错的性能。
示意图如下：
这样可以将运算量降低D_k^2倍左右。

Aggregated Residual Transformations for Deep Neural Networks
作者提出，在传统的ResNet的基础上，以往的方法只往两个方向进行研究，一个深度，一个宽度，但是深度加深，模型训练难度更大，宽度加宽，模型复杂度更高，计算量更大，都在不同的程度上增加了资源的损耗，因此作者从一个新的维度：Cardinality（本文中应该为path的数量）来对模型进行考量，作者在ResNet的基础上提出了一种新的结构，ResNeXt：
上图中两种结构计算量相近，但是右边结构的性能更胜一筹（Cardinality更大）。 
以上三种结构等价，因此可以通过group的形式来实现ResNeXt。其实ResNeXt和mobilenet等结构性质很相近，都是通过group的操作，在维度相同时降低复杂度，或者在复杂度相同时增加维度，然后再通过1*1的卷积将所有通道的信息再融合起来。因此也可以看成是一种精简模型结构的方式。目前已开源。

ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
作者提出，虽然MobileNet、ResNeXt等网络能够大大的降低模型的复杂度，并且也能保持不错的性能，但是1*1卷积的计算消耗还是比较大的，比如在ResNeXt中，一个模块中1*1卷积就占据了93%的运算量，而在MobileNet中更是占到了94.86%，因此作者希望在这个上面进一步降低计算量，在1*1的卷积上也采用group的操作，但是group的操作就无法整合所有通道之间的信息，因此作者就想出了一种channel shuffle的方法，如下图：
这样就可以将不同通道之间的信息融合起来，同时还降低了模型的计算量，ShuffleNet的模块如下：
作者使用了不同的group数进行实验，发现越小的模型，group数量越多性能越好。这是因为在模型大小一样的情况下，group数量越多，feature map的channel数越多，对于小的模型，channel数量对于性能提升更加重要。

方法小结
本节的三篇论文都是对精细模型的设计方法，直接搭建出参数运算量小的模型，适用于嵌入式平台，但是总体性能和目前性能较好的大模型还有一定差距，无法利用现有的大模型，而且多group的模型在caffe上训练起来较慢。

结论
目前关于模型压缩的方法有很多，本文列出的几种方法都是实现起来相对较简单，效果也较明显。其中基于核稀疏化的方法，主要是在参数更新时增加额外的惩罚项，来诱导核的稀疏化，然后就可以利用裁剪或者稀疏矩阵的相关操作来实现模型的压缩；基于模型裁剪的方法，主要是对已训练好的网络进行压缩，往往就是寻找一种更加有效的评价方式，将不重要的参数剔除，以达到模型压缩的目的，这种压缩方法实现简单，尤其是regular的方式，裁剪效率最高，但是如何寻找一个最有效的评价方式是最重要的。基于迁移学习的方法，利用一个性能好的教师网络来监督学生网络进行学习，大大降低了简单网络学习到不重要信息的比例，提高了参数的利用效率，也是目前用的较多的方法。基于精细模型设计的方法，模型本身体积小，运行速度快，性能也不错，目前小的高效模型也开始广泛运用在各种嵌入式平台中。
总的来说，以上几种方法，包括其他的方法，可以结合使用，比如说基于核稀疏化方法中的第一篇文章，先对参数进行结构化的限制，使得参数裁剪起来更加容易，然后再选择合适的裁剪方法，考虑不同的评价标准以及裁剪策略，比如基于模型裁剪方法中的第二、三篇文章，并在裁剪过程中充分考虑参数量、计算量、带宽等需求，以及不同硬件平台特性，在模型的性能、压缩、以及平台上的加速很好的进行权衡，才能够达到更好的效果。
